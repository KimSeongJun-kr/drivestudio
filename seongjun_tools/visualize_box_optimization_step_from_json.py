import argparse
import numpy as np
from typing import Tuple, List, Dict, Optional, Any
import json
import os
from pathlib import Path
from pyquaternion import Quaternion
import open3d as o3d
from open3d.visualization import rendering
from open3d.visualization.rendering import Camera as O3DCamera  # type: ignore
import torch
import glob
import re
from PIL import Image, ImageDraw, ImageFont
from collections import defaultdict

from nuscenes import NuScenes
from nuscenes.eval.detection.data_classes import DetectionBox
from nuscenes.eval.common.data_classes import EvalBoxes
from nuscenes.eval.common.loaders import load_prediction
from nuscenes.eval.detection.config import config_factory
from nuscenes.utils.data_classes import LidarPointCloud
from scipy.spatial.transform import Rotation as R

# ===========================
# Open3D ì‹œê°í™” ìœ í‹¸ë¦¬í‹°
# ===========================

# Open3D LineSet ìƒì„±ì„ ìœ„í•œ ì—ì§€ ì¸ë±ìŠ¤ (12ê°œ)
OPEN3D_BOX_LINES = [
    [0, 1], [1, 2], [2, 3], [3, 0],  # ì•„ë˜ë©´
    [4, 5], [5, 6], [6, 7], [7, 4],  # ìœ„ë©´
    [0, 4], [1, 5], [2, 6], [3, 7]   # ì˜†ë©´
]

def get_box_corners(translation, size, rotation):
    """3D ë°•ìŠ¤ì˜ 8ê°œ ê¼­ì§“ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        translation: [x, y, z] ì¤‘ì‹¬ì 
        size: [width, length, height] í¬ê¸°
        rotation: [w, x, y, z] ì¿¼í„°ë‹ˆì–¸
        
    Returns:
        8x3 numpy array: 8ê°œ ê¼­ì§“ì ì˜ ì¢Œí‘œ
    """
    w, l, h = size
    
    # ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ 8ê°œ ê¼­ì§“ì  (ì¤‘ì‹¬ì´ ì›ì )
    corners_local = np.array([
        [-l/2, -w/2, -h/2],  # 0: ì¢Œí•˜í›„
        [l/2, -w/2, -h/2],   # 1: ìš°í•˜í›„
        [l/2, w/2, -h/2],    # 2: ìš°ìƒí›„
        [-l/2, w/2, -h/2],   # 3: ì¢Œìƒí›„
        [-l/2, -w/2, h/2],   # 4: ì¢Œí•˜ì „
        [l/2, -w/2, h/2],    # 5: ìš°í•˜ì „
        [l/2, w/2, h/2],     # 6: ìš°ìƒì „
        [-l/2, w/2, h/2]     # 7: ì¢Œìƒì „
    ])
    
    # pyquaternionì„ ì‚¬ìš©í•˜ì—¬ íšŒì „ ì ìš©
    q = Quaternion(rotation)  # [w, x, y, z] ìˆœì„œ
    rotation_matrix = q.rotation_matrix
    corners_rotated = (rotation_matrix @ corners_local.T).T
    
    # í‰í–‰ì´ë™ ì ìš©
    corners_world = corners_rotated + np.array(translation)
    
    return corners_world

def create_open3d_sphere(center: np.ndarray, radius: float, color: Tuple[float, float, float]) -> o3d.geometry.TriangleMesh:
    """ì§€ì •í•œ ì¤‘ì‹¬ê³¼ ìƒ‰ìƒì˜ êµ¬(Sphere) Meshë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=radius)
    sphere.translate(center)
    sphere.paint_uniform_color(color)
    return sphere

def create_open3d_box(corners: np.ndarray, color: Tuple[float, float, float]) -> o3d.geometry.TriangleMesh:
    """8ê°œ ê¼­ì§“ì  ì •ë³´ë¡œë¶€í„° Open3D ë‘êº¼ìš´ ì„ ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ìœ¡ë©´ì²´ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì„  ë‘ê»˜ ì„¤ì •
    line_radius = 0.05  # ì„ ì˜ ë°˜ì§€ë¦„ (ë‘ê»˜ ì¡°ì ˆ)
    
    # ëª¨ë“  ì‹¤ë¦°ë”ë¥¼ í•©ì¹  ë©”ì‰¬
    combined_mesh = o3d.geometry.TriangleMesh()
    
    # 12ê°œì˜ ëª¨ì„œë¦¬ì— ëŒ€í•´ ì‹¤ë¦°ë” ìƒì„±
    for line_indices in OPEN3D_BOX_LINES:
        start_point = corners[line_indices[0]]
        end_point = corners[line_indices[1]]
        
        # ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°
        line_vector = end_point - start_point
        line_length = np.linalg.norm(line_vector)
        
        if line_length < 1e-6:  # ë„ˆë¬´ ì§§ì€ ì„ ì€ ê±´ë„ˆë›°ê¸°
            continue
            
        # ì‹¤ë¦°ë” ìƒì„± (Zì¶• ë°©í–¥ìœ¼ë¡œ ìƒì„±ë¨)
        cylinder = o3d.geometry.TriangleMesh.create_cylinder(
            radius=line_radius, 
            height=line_length,
            resolution=8  # ì‹¤ë¦°ë”ì˜ í•´ìƒë„ (ë‚®ìœ¼ë©´ ì„±ëŠ¥ í–¥ìƒ)
        )
        
        # ì‹¤ë¦°ë”ë¥¼ ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ íšŒì „ì‹œí‚¤ê¸°
        # Zì¶• ë‹¨ìœ„ë²¡í„°
        z_axis = np.array([0, 0, 1])
        # ì„ ì˜ ë°©í–¥ ë²¡í„°
        line_direction = line_vector / line_length
        
        # íšŒì „ì¶• ê³„ì‚° (ì™¸ì )
        rotation_axis = np.cross(z_axis, line_direction)
        rotation_axis_norm = np.linalg.norm(rotation_axis)
        
        if rotation_axis_norm > 1e-6:  # í‰í–‰í•˜ì§€ ì•Šì€ ê²½ìš°
            # íšŒì „ê° ê³„ì‚°
            cos_angle = np.dot(z_axis, line_direction)
            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
            
            # íšŒì „ì¶• ì •ê·œí™”
            rotation_axis = rotation_axis / rotation_axis_norm
            
            # íšŒì „ í–‰ë ¬ ìƒì„±
            rotation_matrix = o3d.geometry.get_rotation_matrix_from_axis_angle(
                rotation_axis * angle
            )
            
            # ì‹¤ë¦°ë” íšŒì „
            cylinder.rotate(rotation_matrix, center=(0, 0, 0))
        
        # ì‹¤ë¦°ë”ë¥¼ ì‹œì‘ì ìœ¼ë¡œ ì´ë™ (ì‹¤ë¦°ë” ì¤‘ì‹¬ì´ ì„ ì˜ ì¤‘ì ì´ ë˜ë„ë¡)
        cylinder_center = (start_point + end_point) / 2
        cylinder.translate(cylinder_center)
        
        # ìƒ‰ìƒ ì ìš©
        cylinder.paint_uniform_color(color)
        
        # ë©”ì‰¬ í•©ì¹˜ê¸°
        combined_mesh += cylinder
    
    return combined_mesh

def create_open3d_pointcloud(points: np.ndarray, color: Optional[Tuple[float, float, float]] = None,
                            max_points: int = 50000) -> o3d.geometry.PointCloud:
    """numpy arrayë¡œë¶€í„° Open3D PointCloud ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        points: (N, 3) í˜•íƒœì˜ í¬ì¸íŠ¸ ì¢Œí‘œ
        color: RGB ìƒ‰ìƒ (0~1), Noneì´ë©´ ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
        max_points: ìµœëŒ€ í¬ì¸íŠ¸ ê°œìˆ˜ (ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ)
        
    Returns:
        Open3D PointCloud ê°ì²´
    """
    # í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ
    if len(points) > max_points:
        # ëœë¤ ìƒ˜í”Œë§
        indices = np.random.choice(len(points), max_points, replace=False)
        points = points[indices]
    
    # Open3D PointCloud ìƒì„±
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    
    if color is not None:
        # ë‹¨ì¼ ìƒ‰ìƒ ì ìš©
        pcd.paint_uniform_color(color)
    else:
        # ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒ (íšŒìƒ‰ì¡°)
        distances = np.linalg.norm(points, axis=1)
        max_dist = np.percentile(distances, 95)  # 95 percentileë¡œ ìŠ¤ì¼€ì¼ë§
        normalized_distances = np.clip(distances / max_dist, 0, 1)
        
        # ê±°ë¦¬ê°€ ê°€ê¹Œìš°ë©´ ë°ì€ íšŒìƒ‰, ë©€ë©´ ì–´ë‘ìš´ íšŒìƒ‰
        colors = np.column_stack([1 - normalized_distances * 0.7] * 3)
        pcd.colors = o3d.utility.Vector3dVector(colors)
    
    return pcd

def render_and_save_offscreen(geometries, save_path, w=1920, h=1080, view_width_m: Optional[float] = None, view_height_m: Optional[float] = None):
    """ì˜¤í”„ìŠ¤í¬ë¦° ë Œë”ë§ì„ ìˆ˜í–‰í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        geometries: ë Œë”ë§í•  ê¸°í•˜í•™ì  ê°ì²´ë“¤
        save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        w: ì´ë¯¸ì§€ í­
        h: ì´ë¯¸ì§€ ë†’ì´
        view_width_m: ì»¤ìŠ¤í…€ ë·° í­ (ë¯¸í„°)
        view_height_m: ì»¤ìŠ¤í…€ ë·° ë†’ì´ (ë¯¸í„°)
        
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # ì‹±ê¸€í†¤ ë Œë”ëŸ¬ ê°€ì ¸ì˜¤ê¸° (VRAM ëˆ„ìˆ˜ ë°©ì§€)
        renderer = _get_offscreen_renderer(w, h)

        # í°ìƒ‰ ë°°ê²½ ì ìš© (ì¬ì‚¬ìš© ì‹œ ë§¤ í”„ë ˆì„ ì„¤ì • í•„ìš”)
        try:
            renderer.scene.set_background([1.0, 1.0, 1.0, 2.0])  # RGBA
        except Exception:
            pass

        valid_objects = 0
        for i, g in enumerate(geometries):
            try:
                mat = rendering.MaterialRecord()
                mat.shader = "defaultUnlit"
                mat.base_color = [1.0, 1.0, 1.0, 1.0]
                mat.point_size = 5.0

                renderer.scene.add_geometry(f"g{i}", g, mat)
                valid_objects += 1
            except Exception as e:
                print(f"ê°ì²´ {i} ì¶”ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        if valid_objects == 0:
            print("ì¶”ê°€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ë Œë”ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
        
        # ëª¨ë“  ê°ì²´ë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê³„ì‚°
        all_points = []
        for geom in geometries:
            if hasattr(geom, 'get_axis_aligned_bounding_box'):
                bbox = geom.get_axis_aligned_bounding_box()
                points = np.asarray(bbox.get_box_points())
                all_points.extend(points)
            else:
                print(f"âŒ {geom} ê°ì²´ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° ì‹¤íŒ¨")
         
        if all_points:
            all_points = np.array(all_points)
            center = np.array([0, 0, 0])
            
            # ì¹´ë©”ë¼ ìœ„ì¹˜ ì„¤ì • (ìœ„ì—ì„œ ì•„ë˜ë¡œ ë³´ëŠ” ì‹œì )
            camera_pos = center + [0, 0, 50.]
            
            # ì¹´ë©”ë¼ ì„¤ì • ì ìš©
            try:
                renderer.scene.camera.look_at(center.tolist(),           # ë°”ë¼ë³¼ ì§€ì 
                                camera_pos.tolist(),        # ì¹´ë©”ë¼ ìœ„ì¹˜
                                [0, 1, 0])                 # up ë²¡í„°

                # ì§êµ íˆ¬ì˜(orthographic projection) ê°•ì œ ì ìš©
                try:
                    # ì”¬ì„ ì™„ì „íˆ í¬í•¨í•˜ëŠ” ì •ì‚¬ê°í˜• í¬ë¡­ ê³„ì‚° (ì•½ê°„ì˜ íŒ¨ë”© í¬í•¨)
                    min_bound = np.min(all_points, axis=0)
                    max_bound = np.max(all_points, axis=0)
                    extent = max_bound - min_bound  # [dx, dy, dz]

                    # ë Œë”ëŸ¬ ì¢…íš¡ë¹„ (width / height)
                    aspect = float(w) / float(h)

                    # ì»¤ìŠ¤í…€ ë·° ë²”ìœ„ ì§€ì›
                    if (view_width_m is not None) or (view_height_m is not None):
                        # ì ì–´ë„ í•œ ë©´ì´ ì§€ì •ë¨
                        if (view_width_m is not None) and (view_height_m is not None):
                            half_width  = view_width_m * 0.5
                            half_height = view_height_m * 0.5
                        elif view_width_m is not None:
                            half_width  = view_width_m * 0.5
                            half_height = half_width / aspect
                        else:  # ë†’ì´ë§Œ ì§€ì •ë¨
                            half_height = view_height_m * 0.5  # type: ignore
                            half_width  = half_height * aspect
                    else:
                        # íŒ¨ë”©ì„ í¬í•¨í•œ ìë™ ë²”ìœ„
                        pad = 1.1  # 10% íŒ¨ë”©
                        half_width_req  = extent[0] * 0.5 * pad + 0.5
                        half_height_req = extent[1] * 0.5 * pad + 0.5

                        if (half_width_req / half_height_req) >= aspect:
                            half_width  = half_width_req
                            half_height = half_width / aspect
                        else:
                            half_height = half_height_req
                            half_width  = half_height * aspect

                    left,  right = -half_width,  half_width
                    bottom, top  = -half_height, half_height
                    near, far = 0.1, float(extent[2] + 100.0)

                    # Open3D APIë¥¼ ì‚¬ìš©í•œ ì§êµ íˆ¬ì˜ (enum + frustum)
                    renderer.scene.camera.set_projection(
                        O3DCamera.Projection.Ortho,
                        left,
                        right,
                        bottom,
                        top,
                        near,
                        far,
                    )
                except Exception as e:
                    # í˜„ì¬ Open3D ë²„ì „ì—ì„œ set_projectionì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¡°ìš©íˆ í´ë°±
                    print(f"âš ï¸ ì§êµ íˆ¬ì˜ ì„¤ì • ë¶ˆê°€: {e}")
            except Exception as e:
                print(f"âš ï¸ ì¹´ë©”ë¼ ì„¤ì • ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ ê°ì²´ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì¹´ë©”ë¼ ì„¤ì • ì‚¬ìš©")

        img = renderer.render_to_image()
        
        # ë Œë”ë§ëœ ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬
        if img is None:
            print("âŒ ë Œë”ë§ëœ ì´ë¯¸ì§€ê°€ Noneì…ë‹ˆë‹¤.")
            return False
            
        img_array = np.asarray(img)
        if img_array.size == 0:
            print("âŒ ë Œë”ë§ëœ ì´ë¯¸ì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
                  
        # ì´ë¯¸ì§€ ì €ì¥
        success = o3d.io.write_image(save_path, img)
        
        if success:
            return True
        else:
            print(f"âŒ o3d.io.write_imageê°€ Falseë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ì˜¤í”„ìŠ¤í¬ë¦° ë Œë”ë§ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # ë‹¤ìŒ í”„ë ˆì„ì„ ìœ„í•´ ì§€ì˜¤ë©”íŠ¸ë¦¬ë§Œ ì •ë¦¬ (ë Œë”ëŸ¬ëŠ” ì¬ì‚¬ìš©)
        try:
            if 'renderer' in locals():
                renderer.scene.clear_geometry()
        except Exception:
            pass

# ---------------------------------------------------------------------------
# OffscreenRenderer ì‹±ê¸€í†¤ ê´€ë¦¬ (ë°˜ë³µ ìƒì„±ìœ¼ë¡œ ì¸í•œ VRAM ëˆ„ìˆ˜ ë°©ì§€)
# ---------------------------------------------------------------------------

_GLOBAL_RENDERER: Optional[rendering.OffscreenRenderer] = None  # ì¬ì‚¬ìš©í•  ë Œë”ëŸ¬
_GLOBAL_RENDERER_SIZE: Optional[Tuple[int, int]] = None  # (w, h)

def _get_offscreen_renderer(w: int, h: int) -> rendering.OffscreenRenderer:
    """í•„ìš” ì‹œ ìƒˆë¡œìš´ OffscreenRenderer ë¥¼ ìƒì„±í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

    Open3D <0.18 ë²„ì „ì—ì„œëŠ” OffscreenRenderer ë¥¼ ë°˜ë³µ ìƒì„±í•  ë•Œ GPU ë©”ëª¨ë¦¬ê°€
    í•´ì œë˜ì§€ ì•ŠëŠ” ì´ìŠˆê°€ ìˆì–´, ì‹±ê¸€í†¤ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    global _GLOBAL_RENDERER, _GLOBAL_RENDERER_SIZE

    if _GLOBAL_RENDERER is None or _GLOBAL_RENDERER_SIZE != (w, h):
        # ê¸°ì¡´ ë Œë”ëŸ¬ë¥¼ í•´ì œí•˜ê³  ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
        try:
            if _GLOBAL_RENDERER is not None:
                _GLOBAL_RENDERER.release_resources()  # type: ignore[attr-defined]
        except Exception:
            pass

        _GLOBAL_RENDERER = rendering.OffscreenRenderer(w, h)
        _GLOBAL_RENDERER_SIZE = (w, h)

    # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì§€ì˜¤ë©”íŠ¸ë¦¬ ì´ˆê¸°í™”
    try:
        _GLOBAL_RENDERER.scene.clear_geometry()
    except Exception:
        pass

    return _GLOBAL_RENDERER

# ===========================
# ì¢Œí‘œ ë³€í™˜ ìœ í‹¸ë¦¬í‹°
# ===========================

def get_camera_front_start_pose(nuscenes_dataroot: str, scene_name: str, version: str = 'v1.0-mini') -> Optional[np.ndarray]:
    """NuScenes ì²« ë²ˆì§¸ ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ê°€ì ¸ì˜´ (ì¢Œí‘œ ì •ë ¬ìš©)"""
    try:
        # NuScenes API ì´ˆê¸°í™”
        nusc = NuScenes(version=version, dataroot=nuscenes_dataroot, verbose=False)
        
        # scene ì´ë¦„ìœ¼ë¡œ scene ì°¾ê¸°
        scene = None
        for s in nusc.scene:
            if s['name'] == scene_name:
                scene = s
                break
                
        if scene is None:
            print(f"âš ï¸ Scene '{scene_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        # ì²« ë²ˆì§¸ sample ê°€ì ¸ì˜¤ê¸°
        first_sample_token = scene['first_sample_token']
        first_sample_record = nusc.get('sample', first_sample_token)
        
        # CAM_FRONT (cam_idx=0) ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        cam_name = "CAM_FRONT"
        cam_data = nusc.get('sample_data', first_sample_record['data'][cam_name])
        calib_data = nusc.get('calibrated_sensor', cam_data['calibrated_sensor_token'])
        
        # Extrinsics (camera to ego)
        extrinsics_cam_to_ego = np.eye(4)
        extrinsics_cam_to_ego[:3, :3] = Quaternion(calib_data['rotation']).rotation_matrix
        extrinsics_cam_to_ego[:3, 3] = np.array(calib_data['translation'])
        
        # Get ego pose (ego to world)
        ego_pose_data = nusc.get('ego_pose', cam_data['ego_pose_token'])
        ego_to_world = np.eye(4)
        ego_to_world[:3, :3] = Quaternion(ego_pose_data['rotation']).rotation_matrix
        ego_to_world[:3, 3] = np.array(ego_pose_data['translation'])
        
        # Transform camera extrinsics to world coordinates
        camera_front_start = ego_to_world @ extrinsics_cam_to_ego
        
        return camera_front_start
        
    except Exception as e:
        print(f"âš ï¸ NuScenes APIë¥¼ í†µí•œ ì¹´ë©”ë¼ í¬ì¦ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def transform_pose_to_world(translation: np.ndarray, rotation: np.ndarray, camera_front_start: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """obj_to_camera í¬ì¦ˆë¥¼ obj_to_world í¬ì¦ˆë¡œ ë³€í™˜"""
    # quaternionì„ rotation matrixë¡œ ë³€í™˜
    if len(rotation) == 4:  # quaternion [w, x, y, z]
        rot_matrix = R.from_quat([rotation[1], rotation[2], rotation[3], rotation[0]]).as_matrix()  # scipyëŠ” [x,y,z,w] ìˆœì„œ
    else:
        raise ValueError(f"Unsupported rotation format: {rotation}")
    
    # 4x4 ë³€í™˜ í–‰ë ¬ êµ¬ì„± (obj_to_camera)
    obj_to_camera = np.eye(4)
    obj_to_camera[:3, :3] = rot_matrix
    obj_to_camera[:3, 3] = translation
    
    # obj_to_world = camera_front_start @ obj_to_camera
    obj_to_world = camera_front_start @ obj_to_camera
    
    # world ì¢Œí‘œê³„ì—ì„œ translationê³¼ rotation ì¶”ì¶œ
    world_translation = obj_to_world[:3, 3]
    world_rotation_matrix = obj_to_world[:3, :3]
    
    world_rotation_quat = R.from_matrix(world_rotation_matrix).as_quat()  # [x, y, z, w]
    world_rotation_quat = np.array([world_rotation_quat[3], world_rotation_quat[0], world_rotation_quat[1], world_rotation_quat[2]])  # [w, x, y, z]
    
    return world_translation, world_rotation_quat

# ===========================
# NuScenes ë°•ìŠ¤ ë° LiDAR ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# ===========================

def load_lidar_pointcloud(nusc: 'NuScenes', sample_token: str) -> Optional[np.ndarray]:
    """NuScenes sampleì—ì„œ LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ë¡œë“œí•˜ê³  ego vehicle ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        nusc: NuScenes ê°ì²´
        sample_token: sample token
        
    Returns:
        ego vehicle ì¢Œí‘œê³„ì˜ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ numpy array (N, 3) ë˜ëŠ” None
    """
        
    try:
        # sample ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        sample = nusc.get('sample', sample_token)
        
        # LiDAR ë°ì´í„° í† í° ê°€ì ¸ì˜¤ê¸°
        lidar_token = sample['data']['LIDAR_TOP']
        
        # sample_data ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        sample_data = nusc.get('sample_data', lidar_token)
        
        # LiDAR íŒŒì¼ ê²½ë¡œ
        lidar_path = os.path.join(nusc.dataroot, sample_data['filename'])
        
        if not os.path.exists(lidar_path):
            print(f"âš ï¸ LiDAR íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lidar_path}")
            return None
            
        # LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¡œë“œ (ì„¼ì„œ ì¢Œí‘œê³„)
        pc = LidarPointCloud.from_file(lidar_path)
        
        # LiDAR extrinsic calibration ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        calibrated_sensor = nusc.get('calibrated_sensor', sample_data['calibrated_sensor_token'])
        
        # LiDAR extrinsic: ì„¼ì„œ -> ego vehicle ë³€í™˜
        lidar_translation = np.array(calibrated_sensor['translation'])
        lidar_rotation = Quaternion(calibrated_sensor['rotation'])
        
        # í¬ì¸íŠ¸ë¥¼ ego vehicle ì¢Œí‘œê³„ë¡œ ë³€í™˜
        # Step 1: LiDAR ì„¼ì„œ ì¢Œí‘œê³„ì˜ í¬ì¸íŠ¸ë“¤ (x, y, z)
        points_sensor = pc.points[:3, :]  # (3, N)
        
        # Step 2: LiDAR rotation ì ìš©
        points_rotated = lidar_rotation.rotation_matrix @ points_sensor  # (3, N)
        
        # Step 3: LiDAR translation ì ìš©
        points_ego = points_rotated + lidar_translation.reshape(3, 1)  # (3, N)
        
        # (N, 3) í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return points_ego.T
        
    except Exception as e:
        print(f"âš ï¸ LiDAR ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def add_ego_pose(nusc: 'NuScenes', eval_boxes: 'EvalBoxes') -> 'EvalBoxes':
    """ê° ë°•ìŠ¤ì— ego pose ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        
    for sample_token in eval_boxes.sample_tokens:
        sample_rec = nusc.get('sample', sample_token)
        sd_record = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])
        pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])

        # Get ego pose transformation
        ego_translation_global = np.array(pose_record['translation'])
        ego_rotation_global = Quaternion(pose_record['rotation'])

        for box in eval_boxes[sample_token]:
            # Convert global coordinates to ego vehicle local coordinates
            
            # Step 1: Get relative position vector in global coordinates
            box_translation_global = np.array(box.translation)
            relative_translation_global = box_translation_global - ego_translation_global
            
            # Step 2: Rotate this position vector to ego vehicle's local coordinate system
            ego_translation_array = ego_rotation_global.inverse.rotate(relative_translation_global)
            
            # Step 3: Transform box rotation to ego coordinates
            box_rotation_global = Quaternion(list(box.rotation))  # type: ignore
            ego_rotation = ego_rotation_global.inverse * box_rotation_global
            
            if hasattr(box, 'ego_translation'):
                if isinstance(ego_translation_array, np.ndarray):
                    box.ego_translation = tuple(ego_translation_array.tolist())
                else:
                    box.ego_translation = tuple(ego_translation_array)
                # Add ego_rotation attribute dynamically
                setattr(box, 'ego_rotation', tuple([ego_rotation.w, ego_rotation.x, ego_rotation.y, ego_rotation.z]))  # type: ignore

    return eval_boxes

def add_ego_pose_to_boxes(nusc: 'NuScenes', 
                        frame_boxes: Dict[int, List], 
                        sample_tokens: List[str]) -> Dict[int, List]:
    """ì²´í¬í¬ì¸íŠ¸ ë°•ìŠ¤ ë°ì´í„°ì— ego pose ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        nusc: NuScenes ê°ì²´
        frame_boxes: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ì¶œí•œ í”„ë ˆì„ë³„ ë°•ìŠ¤ ë°ì´í„°
        sample_tokens: NuScenes sample í† í°ë“¤ (ì‹œê°„ìˆœ)
        
    Returns:
        ego poseê°€ ì¶”ê°€ëœ í”„ë ˆì„ë³„ ë°•ìŠ¤ ë°ì´í„°
    """
    if not sample_tokens:
        print("âš ï¸ sample_tokensê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ego pose ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return frame_boxes
    
    # 5ì˜ ë°°ìˆ˜ í”„ë ˆì„ë§Œ ì²˜ë¦¬ (NuScenes í‚¤í”„ë ˆì„)
    keyframe_indices = [i for i in range(0, len(sample_tokens), 1)]  # sample_tokensëŠ” ì´ë¯¸ 5ì˜ ë°°ìˆ˜ë¡œ ì œê³µë¨
    
    for frame_id, boxes_data in frame_boxes.items():
        # frame_idë¥¼ sample_token ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (5ì˜ ë°°ìˆ˜)
        sample_idx = frame_id // 5
        if sample_idx >= len(sample_tokens):
            continue
            
        sample_token = sample_tokens[sample_idx]
        
        try:
            # NuScenesì—ì„œ ego pose ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            sample_rec = nusc.get('sample', sample_token)
            sd_record = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])
            pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])

            # Get ego pose transformation
            ego_translation_global = np.array(pose_record['translation'])
            ego_rotation_global = Quaternion(pose_record['rotation'])
            
            # ê° ë°•ìŠ¤ì— ego pose ë³€í™˜ ì ìš©
            for box_data in boxes_data:
                # ì²´í¬í¬ì¸íŠ¸ ë°•ìŠ¤ì˜ translationê³¼ rotation (ì´ë¯¸ global ì¢Œí‘œê³„ë¡œ ê°€ì •)
                box_translation_global = np.array(box_data['translation'])
                box_rotation_global = Quaternion(box_data['rotation'])  # [w, x, y, z]
                
                # Step 1: Get relative position vector in global coordinates
                relative_translation_global = box_translation_global - ego_translation_global
                
                # Step 2: Rotate this position vector to ego vehicle's local coordinate system
                ego_translation_array = ego_rotation_global.inverse.rotate(relative_translation_global)
                
                # Step 3: Transform box rotation to ego coordinates
                ego_rotation = ego_rotation_global.inverse * box_rotation_global
                
                # ego ì¢Œí‘œê³„ ì •ë³´ë¥¼ ë°•ìŠ¤ ë°ì´í„°ì— ì¶”ê°€
                box_data['ego_translation'] = ego_translation_array.tolist()
                box_data['ego_rotation'] = [ego_rotation.w, ego_rotation.x, ego_rotation.y, ego_rotation.z]
                
        except Exception as e:
            print(f"âš ï¸ Frame {frame_id}ì˜ ego pose ë³€í™˜ ì‹¤íŒ¨: {e}")
            continue
    
    return frame_boxes

def _add_boxes_to_geometries_from_dict(frame_boxes: Dict[int, List], 
                            frame_id: int,
                            color_mapping: Dict[str, Tuple[float, float, float]],
                            use_ego_coordinates: bool = True) -> List:
    """ë°•ìŠ¤ë“¤ì„ ê¸°í•˜í•™ì  ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    geometries = []
    
    if frame_id not in frame_boxes:
        return geometries
    
    for box_data in frame_boxes[frame_id]:
        # ì¢Œí‘œê³„ ì„ íƒ
        if use_ego_coordinates and 'ego_translation' in box_data and 'ego_rotation' in box_data:
            translation = box_data['ego_translation']
            rotation = box_data['ego_rotation']
        else:
            translation = box_data['translation']
            rotation = box_data['rotation']
        
        size = box_data['size']
        node_type = box_data.get('node_type', 'Unknown')
        
        # ë…¸ë“œ íƒ€ì…ë³„ ìƒ‰ìƒ ì„ íƒ
        color = color_mapping.get(node_type, (0.5, 0.5, 0.5))  # ê¸°ë³¸ íšŒìƒ‰
        
        # ë°•ìŠ¤ ìƒì„± ë° ì¶”ê°€
        corners = get_box_corners(translation, size, rotation)
        geometries.append(create_open3d_box(corners, color))
        
        # ì•ë©´ ì¤‘ì‹¬ì  ì‹œê°í™”
        front_center = (corners[1] + corners[6]) / 2 
        geometries.append(create_open3d_sphere(front_center, radius=0.3, color=color))
    
    return geometries

def _add_boxes_to_geometries_from_evalboxes(boxes: Optional['EvalBoxes'], 
                            sample_token: str,
                            color: Tuple[float, float, float],
                            use_ego_coordinates: bool = True) -> List:
    """ë°•ìŠ¤ë“¤ì„ ê¸°í•˜í•™ì  ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    geometries = []
    
    if not boxes or sample_token not in boxes.sample_tokens:
        return geometries
    
    for box in boxes[sample_token]:
        if not (hasattr(box, 'translation') and box.translation is not None and
                hasattr(box, 'size') and box.size is not None and
                hasattr(box, 'rotation') and box.rotation is not None):
            continue
        
        # ì¢Œí‘œê³„ ì„ íƒ
        if use_ego_coordinates and hasattr(box, 'ego_translation') and hasattr(box, 'ego_rotation'):
            translation = box.ego_translation  # type: ignore
            rotation = getattr(box, 'ego_rotation')  # type: ignore
        else:
            translation = box.translation
            rotation = box.rotation
        
        # ë°•ìŠ¤ ìƒì„± ë° ì¶”ê°€
        corners = get_box_corners(translation, box.size, rotation)
        geometries.append(create_open3d_box(corners, color))
        
        # ì•ë©´ ì¤‘ì‹¬ì  ì‹œê°í™”
        front_center = (corners[1] + corners[6]) / 2 
        geometries.append(create_open3d_sphere(front_center, radius=0.3, color=color))
    
    return geometries

# ===========================
# ë°•ìŠ¤ ë°ì´í„° ë¡œë”© ë° ì• ë‹ˆë©”ì´ì…˜ í•¨ìˆ˜ë“¤
# ===========================

def _get_scene_sample_tokens_chronologically(nusc: 'NuScenes', scene_name: str) -> List[str]:
    """Sceneì˜ sample_tokensë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        
    # scene ì°¾ê¸°
    scene_token = None
    for scene in nusc.scene:
        if scene['name'] == scene_name:
            scene_token = scene['token']
            break
    
    if not scene_token:
        return []
    
    # sceneì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œë¶€í„° ì‹œì‘í•˜ì—¬ ì‹œê°„ìˆœìœ¼ë¡œ ìˆ˜ì§‘
    scene = nusc.get('scene', scene_token)
    sample = nusc.get('sample', scene['first_sample_token'])
    scene_sample_tokens = []
    
    while True:
        scene_sample_tokens.append(sample['token'])
        if sample['next'] == '':
            break
        sample = nusc.get('sample', sample['next'])
    
    return scene_sample_tokens

def create_all_sample_animations(box_poses_dir: str, output_dir: str,
                                scene_name: Optional[str] = None,
                                sample_token: Optional[str] = None,
                                pred_boxes: Optional['EvalBoxes'] = None,
                                gt_boxes: Optional['EvalBoxes'] = None,
                                show_lidar: bool = False,
                                nusc: Optional['NuScenes'] = None,
                                max_lidar_points: int = 50000) -> None:
    """ì²´í¬í¬ì¸íŠ¸ë“¤ì—ì„œ ëª¨ë“  ìƒ˜í”Œì˜ ë°•ìŠ¤ ìµœì í™” ê³¼ì •ì„ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # box pose JSON íŒŒì¼ íƒìƒ‰
    box_poses_files = find_boxpose_files(box_poses_dir)

    if not box_poses_files:
        print(f"âŒ box pose JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {box_poses_dir}")
        return
    
    print(f"ğŸ¬ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì‹œì‘: {len(box_poses_files)}ê°œ box pose íŒŒì¼")
    if show_lidar:
        print(f"ğŸ“¡ LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í¬í•¨ (ìµœëŒ€ {max_lidar_points:,}ê°œ í¬ì¸íŠ¸)")
    
    # ìƒ˜í”Œë³„ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
    if scene_name or sample_token:
        # ---------- ìƒˆë¡œìš´ êµ¬í˜„: ì²´í¬í¬ì¸íŠ¸ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•´ ëª¨ë“  ìƒ˜í”Œ ì²˜ë¦¬ ----------

        # 0) ì²˜ë¦¬í•  sample token ëª©ë¡ ê²°ì •
        scene_sample_tokens: List[str] = []
        if scene_name and nusc is not None:
            print(f"ğŸ¬ Scene '{scene_name}'ì˜ ìƒ˜í”Œë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            scene_sample_tokens = _get_scene_sample_tokens_chronologically(nusc, scene_name)
            if not scene_sample_tokens:
                print(f"âŒ Scene '{scene_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤")
                return
            print(f"ğŸ¯ Scene '{scene_name}'ì—ì„œ {len(scene_sample_tokens)}ê°œ ìƒ˜í”Œ í”„ë ˆì„ ë°œê²¬")
        elif sample_token:
            scene_sample_tokens = [sample_token]
            print(f"ğŸ¯ íŠ¹ì • ìƒ˜í”Œë§Œ ì²˜ë¦¬: {sample_token}")
        else:
            print("âš ï¸ scene_nameê³¼ sample_tokenì´ ëª¨ë‘ ì œê³µë˜ì§€ ì•Šì•„ ìƒ˜í”Œì„ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return

        # 1) ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (LiDAR, ì¶œë ¥ ë””ë ‰í† ë¦¬ ë“±)
        sample_contexts: Dict[str, Dict[str, Any]] = {}
        for sample_idx, current_sample_token in enumerate(scene_sample_tokens):
            lidar_points = None
            if show_lidar and nusc is not None:
                lidar_points = load_lidar_pointcloud(nusc, current_sample_token)
                if lidar_points is None:
                    print(f"âŒ LiDAR í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {current_sample_token}")

            sample_output_dir = os.path.join(output_dir, f"sample_{sample_idx:02d}_{current_sample_token}")
            os.makedirs(sample_output_dir, exist_ok=True)

            sample_contexts[current_sample_token] = {
                'idx': sample_idx,
                'lidar_points': lidar_points,
                'output_dir': sample_output_dir,
                'frame_images': []
            }
        print(f"\nğŸ¬ ìƒ˜í”Œ {len(scene_sample_tokens)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")

        # 2) camera_front_start (scene ê¸°ì¤€) í•œ ë²ˆë§Œ ê³„ì‚°
        camera_front_start = None
        if nusc is not None and scene_name:
            camera_front_start = get_camera_front_start_pose(nusc.dataroot, scene_name, nusc.version)
            if camera_front_start is not None:
                print("âœ… Camera front start pose ë¡œë“œ ì™„ë£Œ (1íšŒ)")

        # 3) ë…¸ë“œ íƒ€ì…ë³„ ìƒ‰ìƒ ë§¤í•‘ (ê³ ì •)
        color_mapping = {
            'RigidNodes': (1.0, 0.0, 0.0),
            'SMPLNodes': (1.0, 0.2, 0.0),
            'DeformableNodes': (1.0, 0.25, 0.3),
            'Unknown': (1.0, 0.2, 0.11)
        }

        total_iterations = box_poses_files[-1][0]

        # 4) ì²´í¬í¬ì¸íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©° ëª¨ë“  ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ì¥
        for bp_idx, (iteration, json_path) in enumerate(box_poses_files):
            print(f"\nğŸ”„ Box poses {bp_idx+1}/{len(box_poses_files)} ì²˜ë¦¬ ì¤‘ - Iteration {iteration:06d}")       

            all_frame_boxes = extract_all_boxes_from_json(json_path)
            if all_frame_boxes and nusc is not None and scene_sample_tokens:
                all_frame_boxes = add_ego_pose_to_boxes(nusc, all_frame_boxes, scene_sample_tokens)

            for idx, current_sample_token in enumerate(scene_sample_tokens):
                print(f"{bp_idx+1} / {len(box_poses_files)} ë²ˆì§¸ box pose íŒŒì¼, {idx+1} / {len(scene_sample_tokens)} ë²ˆì§¸ ìƒ˜í”Œ ì¸ë±ìŠ¤ ì²˜ë¦¬ì¤‘...")
                ctx = sample_contexts[current_sample_token]
                sample_idx = ctx['idx']
                sample_output_dir = ctx['output_dir']
                lidar_points = ctx['lidar_points']

                frame_path = os.path.join(sample_output_dir, f"frame_{bp_idx:02d}_iter_{iteration:06d}.png")
                if os.path.exists(frame_path):
                    print(f"skip frame: {frame_path}")
                    ctx['frame_images'].append(frame_path)
                    continue

                # --------- ì‹œê°í™”ìš© Geometry ìƒì„± ---------
                geometries = [o3d.geometry.TriangleMesh.create_coordinate_frame(size=5.0)]

                if lidar_points is not None:
                    lidar_pcd = create_open3d_pointcloud(lidar_points, color=(0.25, 0.25, 0.25), max_points=max_lidar_points)
                    geometries.append(lidar_pcd)

                if all_frame_boxes:
                    frame_id = sample_idx * 5  # 5ì˜ ë°°ìˆ˜ë¡œ ë§¤í•‘
                    box_geometries = _add_boxes_to_geometries_from_dict(
                        all_frame_boxes,
                        frame_id,
                        color_mapping,
                        use_ego_coordinates=True
                    )
                    geometries.extend(box_geometries)

                # pred / gt boxes
                if pred_boxes and current_sample_token in pred_boxes.sample_tokens:
                    geometries.extend(_add_boxes_to_geometries_from_evalboxes(pred_boxes, current_sample_token, (0.0, 0.0, 1.0)))
                if gt_boxes and current_sample_token in gt_boxes.sample_tokens:
                    geometries.extend(_add_boxes_to_geometries_from_evalboxes(gt_boxes, current_sample_token, (0.0, 0.0, 0.0)))

                # ë Œë”ë§ ë° ì €ì¥
                success = render_and_save_offscreen(geometries, frame_path, w=1920, h=1080, view_width_m=100)
                if success and os.path.exists(frame_path) and os.path.getsize(frame_path) > 1000:
                    if add_text_overlay_to_image(frame_path, scene_name if scene_name else "Unknown Scene", sample_idx, iteration, total_iterations):
                        ctx['frame_images'].append(frame_path)
                    else:
                        ctx['frame_images'].append(frame_path)
                else:
                    print(f"  âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {frame_path}")

        # 5) GIF ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ í›„)
        for current_sample_token, ctx in sample_contexts.items():
            if ctx['frame_images']:
                animation_name = f"box_optimization_sample_{ctx['idx']:02d}_{current_sample_token}.gif"
                create_gif_animation_from_files(ctx['frame_images'], output_dir, animation_name)
                print(f"  âœ… ìƒ˜í”Œ {current_sample_token} ì• ë‹ˆë©”ì´ì…˜ ì™„ë£Œ: {ctx['output_dir']}")
            else:
                print(f"  âŒ ìƒ˜í”Œ {current_sample_token}ì— ëŒ€í•œ ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")

        print(f"\nğŸ‰ ëª¨ë“  ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ! ê²°ê³¼: {output_dir}")
        return  # ê¸°ì¡´ ë¡œì§ ì‹¤í–‰ ë°©ì§€

    print(f"\nğŸ‰ ëª¨ë“  ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ! ê²°ê³¼: {output_dir}")

def add_text_overlay_to_image(image_path: str, scene_name: str, frame_idx: int, 
                             iteration: int, total_iterations: int) -> bool:
    """ì´ë¯¸ì§€ ì¢Œì¸¡ ìƒë‹¨ì— ì •ë³´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(image_path)
        draw = ImageDraw.Draw(img)
        font_size = 50
        
        # í°íŠ¸ ì„¤ì • (ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©, í¬ê¸° ì¡°ì •)
        try:
            # ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì‹œë„
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
        except:
            try:
                # ëŒ€ì•ˆ í°íŠ¸ ì‹œë„
                font = ImageFont.truetype("arial.ttf", font_size)
            except:
                try:
                    # ë‹¤ë¥¸ ì‹œìŠ¤í…œ í°íŠ¸ë“¤ ì‹œë„
                    system_fonts = [
                        "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
                        "/System/Library/Fonts/Arial.ttf",  # macOS
                        "/Windows/Fonts/arial.ttf",  # Windows
                        "/usr/share/fonts/TTF/arial.ttf"  # Some Linux
                    ]
                    font = None
                    for font_path in system_fonts:
                        try:
                            font = ImageFont.truetype(font_path, font_size)
                            break
                        except:
                            continue
                    
                    if font is None:
                        raise Exception("No system fonts found")
                        
                except:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© (í¬ê¸° ì¡°ì • ë¶ˆê°€)
                    font = ImageFont.load_default()
                    print(f"âš ï¸ ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (í¬ê¸° ì¡°ì • ë¶ˆê°€)")
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© êµ¬ì„± (ê°€ë¡œ ë°°ì¹˜ìš©)
        text_parts = [
            f"Scene: {scene_name}",
            f"Frame: {frame_idx}",
            f"Iteration: {iteration:,} / {total_iterations:,}"
        ]
        
        # í…ìŠ¤íŠ¸ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜í•˜ê¸° ìœ„í•´ êµ¬ë¶„ìë¡œ ì—°ê²°
        full_text = " | ".join(text_parts)
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì„¤ì • (ì¢Œì¸¡ ìƒë‹¨)
        x_offset = 20
        y_offset = 20
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì˜ í¬ê¸° ì¸¡ì •
        bbox = draw.textbbox((0, 0), full_text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        # ë°˜íˆ¬ëª… ë°°ê²½ ë°•ìŠ¤ (ê²€ì€ìƒ‰, 70% íˆ¬ëª…ë„)
        background_box = [
            x_offset - 10, 
            y_offset - 10, 
            x_offset + text_width + 20, 
            y_offset + text_height + 20
        ]
        
        # ë°°ê²½ ë°•ìŠ¤ë¥¼ ìœ„í•œ ë³„ë„ ì´ë¯¸ì§€ ìƒì„± í›„ ì•ŒíŒŒ ë¸”ë Œë”©
        overlay = Image.new('RGBA', img.size)
        overlay_draw = ImageDraw.Draw(overlay)
        overlay_draw.rectangle(background_box, fill=(0, 0, 0, 180))  # ê²€ì€ìƒ‰, 70% ë¶ˆíˆ¬ëª…
        
        # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì˜¤ë²„ë ˆì´ í•©ì„±
        if img.mode != 'RGBA':
            img = img.convert('RGBA')
        img = Image.alpha_composite(img, overlay)
        draw = ImageDraw.Draw(img)
        
        # í°ìƒ‰ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ê°€ë¡œ í•œ ì¤„)
        draw.text((x_offset, y_offset), full_text, fill=(255, 255, 255, 255), font=font)
        
        # RGB ëª¨ë“œë¡œ ë³€í™˜ í›„ ì €ì¥
        if img.mode == 'RGBA':
            img = img.convert('RGB')
        
        img.save(image_path)
        return True
        
    except Exception as e:
        print(f"âš ï¸ í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return False

def create_gif_animation_from_files(frame_files: List[str], output_dir: str, base_name: str) -> None:
    """í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ GIF ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
    try:
        if not frame_files:
            print(f"âŒ í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"ğŸ”— {len(frame_files)}ê°œ í”„ë ˆì„ì„ GIFë¡œ ê²°í•© ì¤‘...")
        
        # ì´ë¯¸ì§€ë“¤ ë¡œë“œ
        images = []
        for frame_file in frame_files:
            if os.path.exists(frame_file) and os.path.getsize(frame_file) > 1000:  # ìµœì†Œ 1KB ì´ìƒ
                img = Image.open(frame_file)
                images.append(img)
            else:
                print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë ˆì„ ê±´ë„ˆë›°ê¸°: {frame_file}")
        
        if not images:
            print(f"âŒ ìœ íš¨í•œ í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # GIF ê²½ë¡œ ìƒì„±
        gif_path = os.path.join(output_dir, base_name)
        
        # GIF ìƒì„±
        images[0].save(
            gif_path,
            save_all=True,
            append_images=images[1:],
            duration=125,
            loop=0
        )
        
        print(f"âœ… GIF ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ: {gif_path}")
        print(f"   ğŸ“Š í”„ë ˆì„ ìˆ˜: {len(images)}ê°œ")
        print(f"   ğŸ“‚ í¬ê¸°: {os.path.getsize(gif_path):,} bytes")
        
    except Exception as e:
        print(f"âŒ GIF ìƒì„± ì‹¤íŒ¨: {e}")

def find_boxpose_files(boxpose_dir: str) -> List[Tuple[int, str]]:
    """box_poses ë””ë ‰í† ë¦¬ì—ì„œ box_poses_*.json íŒŒì¼ì„ iteration ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì„œ ì°¾ìŠµë‹ˆë‹¤."""

    boxpose_files: List[Tuple[int, str]] = []

    pattern = os.path.join(boxpose_dir, "box_poses_*.json")
    files = glob.glob(pattern)

    for file_path in files:
        filename = os.path.basename(file_path)
        # box_poses_01000.json â†’ 1000 ì¶”ì¶œ
        match = re.search(r'box_poses_(\d+)\.json', filename)
        if match:
            iteration = int(match.group(1))
            boxpose_files.append((iteration, file_path))

    # iteration ìˆœìœ¼ë¡œ ì •ë ¬
    boxpose_files.sort(key=lambda x: x[0])

    print(f"ğŸ“ ì°¾ì€ box pose íŒŒì¼ë“¤ ({len(boxpose_files)}ê°œ):")
    for iteration, file_path in boxpose_files:
        print(f"  - Iteration {iteration:06d}: {os.path.basename(file_path)}")

    return boxpose_files

def extract_all_boxes_from_json(json_path: str) -> Optional[Dict[int, List]]:
    """box_poses_*.json íŒŒì¼ì—ì„œ ëª¨ë“  í”„ë ˆì„ì˜ ë°•ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""

    print(f"ğŸ” JSON ë¡œë”©: {os.path.basename(json_path)}")
    with open(json_path, 'r') as f:
        data = json.load(f)

    if not isinstance(data, dict) or 'results' not in data:
        print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ JSON êµ¬ì¡°: {json_path}")
        return None

    results = data['results']

    frame_boxes: Dict[int, List] = {}

    total_boxes = 0
    for frame_id_str, boxes in results.items():
        try:
            frame_id = int(frame_id_str)
        except ValueError:
            print(f"âŒ frame idê°€ ì •ìˆ˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤: {frame_id_str}")
            continue

        # box ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥ (í•„ìš” ì‹œ deep copy)
        frame_boxes[frame_id] = boxes
        total_boxes += len(boxes)

    print(f"âœ… JSONì—ì„œ ì¶”ì¶œëœ ì´ ë°•ìŠ¤ ìˆ˜: {total_boxes}ê°œ ({len(frame_boxes)}ê°œ í”„ë ˆì„)")

    return frame_boxes

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Visualize bounding box optimization steps as animation")
    
    # ì• ë‹ˆë©”ì´ì…˜ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--box_poses_dir",
        type=str,
        default="/workspace/drivestudio/output/test_250703/test_try1/box_poses",
        help="Directory containing box pose JSON files (box_poses_*.json)"
    )
    parser.add_argument(
        "--scene_name",
        type=str,
        default='scene-0103',
        help="Scene name to animate boxes optimization (e.g., 'scene-0061', 'scene-0103', 'scene-0553', 'scene-0655', "
                                                "'scene-0757', 'scene-0796', 'scene-0916', 'scene-1077', "
                                                "'scene-1094', 'scene-1100')",
    )
    parser.add_argument(
        "--sample_token",
        type=str,
        default=None,
        help="Specific sample token to visualize (if provided, only this sample will be processed)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Directory to save animation frames and final GIF"
    )
    
    # pred_boxes, gt_boxes ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--pred_boxes",
        type=str,
        default='/workspace/drivestudio/output/ceterpoint_pose/results_nusc_matched_pred_real_selected_tar1.json',
        help="Path to prediction boxes json file",
    )
    parser.add_argument(
        "--gt_boxes",
        type=str,
        default='/workspace/drivestudio/output/ceterpoint_pose/results_nusc_gt_pred_selected_src.json',
        help="Path to ground truth boxes json file",
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v1.0-mini",
        help="NuScenes version",
    )
    parser.add_argument(
        "--dataroot",
        type=str,
        default="/workspace/drivestudio/data/nuscenes/raw",
        help="NuScenes dataroot",
    )
    parser.add_argument(
        "--verbose",
        type=bool,
        default=False,
        help="Verbose",
    )
    
    # LiDAR ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--show_lidar",
        type=bool,
        default=True,
        help="LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œë„ í•¨ê»˜ ì‹œê°í™”"
    )
    parser.add_argument(
        "--max_lidar_points",
        type=int,
        default=500000,
        help="ì‹œê°í™”í•  ìµœëŒ€ LiDAR í¬ì¸íŠ¸ ê°œìˆ˜"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ ë°”ìš´ë”© ë°•ìŠ¤ ìµœì í™” ì‹œê°í™” ë„êµ¬")
    print(f"ğŸ“ Box poses ë””ë ‰í† ë¦¬: {args.box_poses_dir}")
    print(f"ğŸ¬ Scene: {args.scene_name}")
    print(f"ğŸ¯ ìƒ˜í”Œ í† í°: {args.sample_token if args.sample_token else 'ëª¨ë“  ìƒ˜í”Œ'}")
    print(f"ğŸ“¡ LiDAR ì‹œê°í™”: {'í™œì„±í™”' if args.show_lidar else 'ë¹„í™œì„±í™”'}")
    
    # NuScenes ì´ˆê¸°í™” (scene_name, pred_boxes, gt_boxes, LiDAR ê¸°ëŠ¥ìš©)
    nusc = None
    pred_boxes = None
    gt_boxes = None
    
    if args.scene_name or args.pred_boxes or args.gt_boxes or args.show_lidar:
        print("ğŸ“Š NuScenes ë°ì´í„° ë¡œë”© ì¤‘...")
        nusc = NuScenes(version=args.version, dataroot=args.dataroot, verbose=args.verbose)
        config = config_factory('detection_cvpr_2019')
        
        # Load prediction boxes if provided
        if args.pred_boxes and os.path.exists(args.pred_boxes):
            print(f"ğŸ“Š Prediction boxes ë¡œë”© ì¤‘: {args.pred_boxes}")
            pred_boxes, _ = load_prediction(args.pred_boxes, 
                                           config.max_boxes_per_sample, 
                                           DetectionBox,
                                           verbose=args.verbose)
            pred_boxes = add_ego_pose(nusc, pred_boxes)
        
        # Load ground truth boxes if provided
        if args.gt_boxes and os.path.exists(args.gt_boxes):
            print(f"ğŸ“Š Ground truth boxes ë¡œë”© ì¤‘: {args.gt_boxes}")
            gt_boxes, _ = load_prediction(args.gt_boxes, 
                                         config.max_boxes_per_sample, 
                                         DetectionBox,
                                         verbose=args.verbose)
            gt_boxes = add_ego_pose(nusc, gt_boxes)
    
    # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
    if not os.path.exists(args.box_poses_dir):
        print(f"âŒ Box poses ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.box_poses_dir}")
        return
    
    if args.output_dir is None:
        args.output_dir = os.path.join(args.box_poses_dir, "box_optimization_animation")
    
    create_all_sample_animations(
        box_poses_dir=args.box_poses_dir,
        output_dir=args.output_dir,
        scene_name=args.scene_name,
        sample_token=args.sample_token,
        pred_boxes=pred_boxes,
        gt_boxes=gt_boxes,
        show_lidar=args.show_lidar,
        nusc=nusc,
        max_lidar_points=args.max_lidar_points
    )

if __name__ == "__main__":
    main()
    
